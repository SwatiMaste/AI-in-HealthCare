{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwvAvHflKk-A"
      },
      "source": [
        "# Fine-tuning TinyLlama on MedMCQA with LoRA\n",
        "This notebook fine-tunes `TinyLlama/TinyLlama-1.1B-Chat-v1.0` on a **subset (3k)** of MedMCQA using **LoRA**.\n",
        "Link to SFT+LoRA code: https://colab.research.google.com/drive/1UfRcH8FcByb3mAV8sNTiq7r0Fxfa3ur5?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thDLC8cmKk-B",
        "outputId": "4094537f-54fa-4d53-bfcc-8036cf3fb791"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: trl in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (0.17.0)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from trl) (1.6.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from trl) (3.5.0)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from trl) (14.0.0)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from trl) (4.51.3)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from accelerate>=0.34.0->trl) (2.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from accelerate>=0.34.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from accelerate>=0.34.0->trl) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from accelerate>=0.34.0->trl) (2.5.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from accelerate>=0.34.0->trl) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.11.18)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from rich->trl) (2.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from aiohttp->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from aiohttp->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from aiohttp->datasets>=3.0.0->trl) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from aiohttp->datasets>=3.0.0->trl) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from aiohttp->datasets>=3.0.0->trl) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from aiohttp->datasets>=3.0.0->trl) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (75.8.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers datasets accelerate peft bitsandbytes trl sentencepiece huggingface_hub\n",
        "!pip install -U trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KfhoqNMKk-C",
        "outputId": "835fc12f-1f2f-4ecd-f112-c994e9df7281"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTConfig,SFTTrainer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrZyT_v2Fesz",
        "outputId": "b4816aa5-9574-47bb-ce74-65686800eb37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ MPS enabled on Apple Silicon\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Set device to MPS if available, else fallback to CPU\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"‚úÖ MPS enabled on Apple Silicon\")\n",
        "elif torch.cuda.is_available():  # (in case you ever run on a CUDA machine)\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"‚úÖ CUDA GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"‚ö†Ô∏è  Running on CPU\")\n",
        "\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h7kvR3PKk-C"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "DATASET_NAME = \"openlifescienceai/medmcqa\"\n",
        "#OUTPUT_DIR = \"/content/drive/MyDrive/tinyllama_medmcqa\"\n",
        "OUTPUT_DIR = \"tinyllama_medmcqa/output\"\n",
        "LORA_TARGET_MODULES = [\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # attention projection\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\"     # MLP feed-forward projection\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiLp3JoDFes0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "torch.mps.empty_cache()\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.to(device)  # Move model to Apple GPU\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpWwV5n_Fes0",
        "outputId": "52e9dff7-5cbc-48b7-de49-40cf94fbf18c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample prompt:\n",
            "You are a helpful medical assistant.\n",
            "\n",
            "Question: Amount of heat that is required to change boiling water into vapor is referred to as\n",
            "Options:\n",
            "0: Latent Heat of vaporization\n",
            "1: Latent Heat of sublimation\n",
            "2: Latent Heat of condensation\n",
            "3: Latent heat of fusion\n",
            "Answer with only the number (0, 1, 2, or 3).\n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def format_instruction_eval(sample):\n",
        "    question = sample['question']\n",
        "    options = \"\\n\".join([\n",
        "        f\"0: {sample['opa']}\",\n",
        "        f\"1: {sample['opb']}\",\n",
        "        f\"2: {sample['opc']}\",\n",
        "        f\"3: {sample['opd']}\"\n",
        "    ])\n",
        "    prompt = (\n",
        "        f\"You are a helpful medical assistant.\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Options:\\n{options}\\n\"\n",
        "        f\"Answer with only the number (0, 1, 2, or 3).\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "    return {\"text\": prompt, \"cop\": sample[\"cop\"]}\n",
        "\n",
        "def format_instruction_train_chat(sample):\n",
        "    question = sample['question']\n",
        "    options = \"\\n\".join([\n",
        "        f\"0: {sample['opa']}\",\n",
        "        f\"1: {sample['opb']}\",\n",
        "        f\"2: {sample['opc']}\",\n",
        "        f\"3: {sample['opd']}\"\n",
        "    ])\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are a helpful medical assistant.\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Options:\\n{options}\\n\"\n",
        "        f\"Answer with only the number (0, 1, 2, or 3).\\n\"\n",
        "        f\"Answer: {sample['cop']}\"\n",
        "    )\n",
        "\n",
        "    return {\"text\": prompt, \"cop\": str(sample[\"cop\"])}\n",
        "\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "DATASET_NAME = \"openlifescienceai/medmcqa\"\n",
        "TRAIN_SAMPLE_SIZE = 3000\n",
        "EVAL_SAMPLE_SIZE = 1000\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "\n",
        "# Format and sample training set\n",
        "train_dataset = dataset[\"train\"] \\\n",
        "    .shuffle(seed=42) \\\n",
        "    .select(range(TRAIN_SAMPLE_SIZE)) \\\n",
        "    .map(format_instruction_train_chat)\n",
        "\n",
        "# with answer\n",
        "\n",
        "# Format and sample validation set\n",
        "val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(EVAL_SAMPLE_SIZE)).map(format_instruction_eval) # without answer\n",
        "\n",
        "# For evaluation use\n",
        "eval_dataset = val_dataset\n",
        "print(\"Sample prompt:\")\n",
        "print(eval_dataset[0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyuJeselFes1"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, dataset):\n",
        "    logging = True\n",
        "    correct = 0\n",
        "\n",
        "    for i, sample in enumerate(tqdm(dataset)):\n",
        "        prompt = sample[\"text\"]\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH).to(model.device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=2,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            temperature=0.0,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Just search for the answer anywhere in the decoded output\n",
        "        #match = re.search(r\"\\b([0-3])\\b\", decoded)\n",
        "        #predicted_answer = int(match.group(1)) if match else -1\n",
        "\n",
        "        match = None\n",
        "        matches = list(re.finditer(r\"\\b([0-3])\\b\", decoded))\n",
        "        if matches:\n",
        "            match = matches[-1]  # pick last match\n",
        "            predicted_answer = int(match.group(1)) if match else -1\n",
        "\n",
        "        if predicted_answer == sample[\"cop\"]:\n",
        "            correct += 1\n",
        "\n",
        "        if logging:\n",
        "            print(\"üîé Prompt:\", prompt)\n",
        "            print(\"üì§ Full Decoded Output:\", decoded)\n",
        "            print(\"üî¢ Predicted:\", predicted_answer, \"| Actual:\", sample[\"cop\"])\n",
        "            logging = False  # Show only once\n",
        "\n",
        "    return correct / len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz6MpXg7Kk-D",
        "outputId": "04099ead-f301-42ee-bc4f-3804f5600e57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating base TinyLlama...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/1000 [00:00<03:05,  5.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîé Prompt: You are a helpful medical assistant.\n",
            "\n",
            "Question: Amount of heat that is required to change boiling water into vapor is referred to as\n",
            "Options:\n",
            "0: Latent Heat of vaporization\n",
            "1: Latent Heat of sublimation\n",
            "2: Latent Heat of condensation\n",
            "3: Latent heat of fusion\n",
            "Answer with only the number (0, 1, 2, or 3).\n",
            "Answer:\n",
            "üì§ Full Decoded Output: You are a helpful medical assistant.\n",
            "\n",
            "Question: Amount of heat that is required to change boiling water into vapor is referred to as\n",
            "Options:\n",
            "0: Latent Heat of vaporization\n",
            "1: Latent Heat of sublimation\n",
            "2: Latent Heat of condensation\n",
            "3: Latent heat of fusion\n",
            "Answer with only the number (0, 1, 2, or 3).\n",
            "Answer: 0\n",
            "üî¢ Predicted: 0 | Actual: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:26<00:00, 11.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base Accuracy: 24.30%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Evaluating base TinyLlama...\")\n",
        "base_acc = evaluate_model(model, tokenizer, eval_dataset)\n",
        "print(f\"Base Accuracy: {base_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MXm5arKKk-E",
        "outputId": "c6dcf160-1da6-4812-c45e-8b02379cbad0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
            "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-21): 22 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=5632, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=5632, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=5632, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTConfig\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 3\n",
        "GRAD_ACC = 4\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "LORA_DROPOUT = 0.05\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    bias=\"none\",\n",
        "    use_rslora=True,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "training_args =  SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=GRAD_ACC,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    logging_steps=10,\n",
        "    eval_steps=200,  # ‚úÖ evaluation will happen every 200 steps\n",
        "    save_steps=1000,\n",
        "    lr_scheduler_type='cosine_with_restarts',\n",
        "    warmup_ratio=0.05,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=0.3,\n",
        "    remove_unused_columns=False,\n",
        "    bf16=torch.backends.mps.is_available(),\n",
        "    save_safetensors=True\n",
        ")\n",
        "torch.mps.empty_cache()\n",
        "model.to(\"mps\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSwCsadvFes6",
        "outputId": "c1764ea8-3e83-4757-efee-d704a8d6987b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'c69ce981-e254-4707-b87a-cb493e8a948e',\n",
              " 'question': 'OPV can be used if vaccine l monitor is showing?',\n",
              " 'opa': 'Colour of outer circle is same as inner square',\n",
              " 'opb': 'Colour of outer circle is darker than inner square',\n",
              " 'opc': 'Colour of outer circle is lighter than inner square',\n",
              " 'opd': 'None of the above',\n",
              " 'cop': 1,\n",
              " 'choice_type': 'multi',\n",
              " 'exp': \"Ans. is 'b' i.e., Colour of outer circle is darker than inner square\",\n",
              " 'subject_name': 'Social & Preventive Medicine',\n",
              " 'topic_name': None,\n",
              " 'text': 'You are a helpful medical assistant.\\n\\nQuestion: OPV can be used if vaccine l monitor is showing?\\nOptions:\\n0: Colour of outer circle is same as inner square\\n1: Colour of outer circle is darker than inner square\\n2: Colour of outer circle is lighter than inner square\\n3: None of the above\\nAnswer with only the number (0, 1, 2, or 3).\\nAnswer: 1'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aknEg7zNFes6",
        "outputId": "0dfe76a1-be7b-4a3f-92d9-9f649d8e79c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:00<00:00, 5750.21 examples/s]\n"
          ]
        }
      ],
      "source": [
        "MAX_SEQ_LENGTH = 256\n",
        "def tokenize_function_masked(example):\n",
        "    text = example[\"text\"]\n",
        "    answer = str(example[\"cop\"])  # e.g. \"1\"\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "    )\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    labels = [-100] * len(input_ids)\n",
        "\n",
        "    # Tokenize the answer string directly\n",
        "    answer_token_ids = tokenizer(answer, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    # Search for the answer token in input_ids\n",
        "    for i in range(len(input_ids) - len(answer_token_ids) + 1):\n",
        "        if input_ids[i:i+len(answer_token_ids)] == answer_token_ids:\n",
        "            for j in range(len(answer_token_ids)):\n",
        "                labels[i + j] = input_ids[i + j]\n",
        "            break  # stop after first match\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "    return tokenized\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function_masked, batched=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECrUdkfAFes7",
        "outputId": "8d4138fb-4745-4fd4-9b45-5a50f9f7761f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded: <s> You are a helpful medical assistant.\n",
            "\n",
            "Question: OPV can be used if vaccine l monitor is showing?\n",
            "Options:\n",
            "0: Colour of outer circle is same as inner square\n",
            "1: Colour of outer circle is darker than inner square\n",
            "2: Colour of outer circle is lighter than inner square\n",
            "3: None of the above\n",
            "Answer with only the number (0, 1, 2, or 3).\n",
            "Answer: 1</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
            "Labels: ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '', '1', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n"
          ]
        }
      ],
      "source": [
        "d = tokenize_function_masked(train_dataset[0])\n",
        "print(\"Decoded:\", tokenizer.decode(d[\"input_ids\"]))\n",
        "print(\"Labels:\", [tokenizer.decode([x]) if x != -100 else '_' for x in d[\"labels\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPOccUuaFes8",
        "outputId": "ad2bca32-d96a-4216-c052-0cb723957a2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:00<00:00, 87829.63 examples/s]\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='561' max='561' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [561/561 40:41, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.260700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.662800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.463200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.448500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.423400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.414200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.426200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.413300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.389000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.449100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.401800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.419200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.436300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.398200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.425500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.383900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.420300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.407800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.398000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.376800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.357400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.335800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.337900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.337500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.320300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.356200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.326500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.335200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.369800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.349800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.336600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.341700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.329600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.319200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.366200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.342100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.326400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.307200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.290100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.307100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.281000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.308400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.272700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.273600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.289500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.276700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.288600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.275700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.281500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.283100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.271300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.287600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.282000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.273000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.261200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.261700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=561, training_loss=0.4029473512567938, metrics={'train_runtime': 2448.0006, 'train_samples_per_second': 3.676, 'train_steps_per_second': 0.229, 'total_flos': 1.441116280848384e+16, 'train_loss': 0.4029473512567938})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from transformers import EarlyStoppingCallback\n",
        "# Initialize the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    peft_config=peft_config,\n",
        "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]  # üëà patience=10\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHVjYwThtlsk",
        "outputId": "69f5a967-5643-4737-8a58-1f62f47fc629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to tinyllama_medmcqa/output/LoRA1\n",
            "total 106752\n",
            "-rw-r--r--  1 jagadeeshbandlamudi  staff   5.0K Apr 26 19:49 README.md\n",
            "-rw-r--r--  1 jagadeeshbandlamudi  staff   866B Apr 26 19:49 adapter_config.json\n",
            "-rw-r--r--  1 jagadeeshbandlamudi  staff    48M Apr 26 19:49 adapter_model.safetensors\n",
            "-rw-r--r--  1 jagadeeshbandlamudi  staff   551B Apr 26 19:49 special_tokens_map.json\n",
            "-rw-r--r--  1 jagadeeshbandlamudi  staff   3.5M Apr 26 19:49 tokenizer.json\n",
            "-rw-r--r--  1 jagadeeshbandlamudi  staff   488K Apr 26 19:49 tokenizer.model\n",
            "-rw-r--r--  1 jagadeeshbandlamudi  staff   1.4K Apr 26 19:49 tokenizer_config.json\n",
            "-rw-r--r--  1 jagadeeshbandlamudi  staff   5.6K Apr 26 19:49 training_args.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model(f\"{OUTPUT_DIR}/LoRA1\")\n",
        "print(\"Model saved to\", f\"{OUTPUT_DIR}/LoRA1\")\n",
        "!ls -lh {OUTPUT_DIR}/LoRA1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pv8049WsYcy",
        "outputId": "d35ac752-5eb7-47f9-962f-ed882afe3af0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model ID: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Adapter path: tinyllama_medmcqa/output/LoRA1\n",
            "Merged model path: tinyllama_medmcqa/output/Merged\n",
            "Model merged and saved to tinyllama_medmcqa/output/Merged\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Setup\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Paths\n",
        "base_model_id = MODEL_NAME\n",
        "adapter_path = f\"{OUTPUT_DIR}/LoRA1\"\n",
        "merged_model_path = f\"{OUTPUT_DIR}/Merged\"\n",
        "\n",
        "print(f\"Base model ID: {base_model_id}\")\n",
        "print(f\"Adapter path: {adapter_path}\")\n",
        "print(f\"Merged model path: {merged_model_path}\")\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if torch.backends.mps.is_available() else torch.float32,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "# Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "# Merge LoRA into base model\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save merged model\n",
        "model.save_pretrained(merged_model_path)\n",
        "tokenizer.save_pretrained(merged_model_path)\n",
        "print(f\"Model merged and saved to {merged_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0wt7rAAFes8",
        "outputId": "2a0a0368-8365-40b2-f7a8-116f7cf30e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "merged_model_path tinyllama_medmcqa/output/Merged\n"
          ]
        }
      ],
      "source": [
        "# === Reload merged model for evaluation ===\n",
        "print(\"merged_model_path\", merged_model_path)\n",
        "mergedModel = AutoModelForCausalLM.from_pretrained(merged_model_path, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(merged_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG_2KfNkFes9",
        "outputId": "28c6f192-89ae-4466-de3d-fa217b93dcee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]/opt/anaconda3/envs/lora-train-312/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "  0%|          | 1/1000 [00:05<1:32:15,  5.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîé Prompt: You are a helpful medical assistant.\n",
            "\n",
            "Question: Amount of heat that is required to change boiling water into vapor is referred to as\n",
            "Options:\n",
            "0: Latent Heat of vaporization\n",
            "1: Latent Heat of sublimation\n",
            "2: Latent Heat of condensation\n",
            "3: Latent heat of fusion\n",
            "Answer with only the number (0, 1, 2, or 3).\n",
            "Answer:\n",
            "üì§ Full Decoded Output: You are a helpful medical assistant.\n",
            "\n",
            "Question: Amount of heat that is required to change boiling water into vapor is referred to as\n",
            "Options:\n",
            "0: Latent Heat of vaporization\n",
            "1: Latent Heat of sublimation\n",
            "2: Latent Heat of condensation\n",
            "3: Latent heat of fusion\n",
            "Answer with only the number (0, 1, 2, or 3).\n",
            "Answer: 1\n",
            "üî¢ Predicted: 1 | Actual: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [07:07<00:00,  2.34it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Model  Accuracy Improvement\n",
            "0        Base     0.243           -\n",
            "1  Fine-tuned     0.322    7.90000%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate\n",
        "ft_acc = evaluate_model(mergedModel, tokenizer, eval_dataset)\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Base\", \"Fine-tuned\"],\n",
        "    \"Accuracy\": [base_acc, ft_acc],\n",
        "    \"Improvement\": [\"-\", f\"{ft_acc - base_acc:.5%}\"]\n",
        "})\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lora-train-312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}